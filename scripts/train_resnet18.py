#!/usr/bin/env python3
"""
ResNet18èº«ä»½åˆ†ç±»å™¨è®­ç»ƒè„šæœ¬
ç”¨äºè®­ç»ƒ10ä¸ªç”¨æˆ·(ID_1åˆ°ID_10)çš„èº«ä»½è¯†åˆ«æ¨¡å‹
"""

import os
import glob
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from tqdm import tqdm
import argparse
from PIL import Image
import json
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class GaitDataset(Dataset):
    """æ­¥æ€æ•°æ®é›†ç±»"""
    
    def __init__(self, image_paths, labels, transform=None, contrastive_mode=False):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.contrastive_mode = contrastive_mode
        
        # ä¸ºå¯¹æ¯”å­¦ä¹ åˆ›å»ºæŒ‰ç±»åˆ«åˆ†ç»„çš„ç´¢å¼•
        if self.contrastive_mode:
            self.class_to_indices = {}
            for idx, label in enumerate(labels):
                if label not in self.class_to_indices:
                    self.class_to_indices[label] = []
                self.class_to_indices[label].append(idx)
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        if self.contrastive_mode:
            return self._get_contrastive_pair(idx)
        else:
            return self._get_single_item(idx)
    
    def _get_single_item(self, idx):
        """è·å–å•ä¸ªå›¾åƒå’Œæ ‡ç­¾"""
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
                
            return image, label
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # è¿”å›é›¶å›¾åƒä½œä¸ºå¤‡ç”¨
            if self.transform:
                zero_image = self.transform(Image.new('RGB', (256, 256), (0, 0, 0)))
            else:
                zero_image = transforms.ToTensor()(Image.new('RGB', (256, 256), (0, 0, 0)))
            return zero_image, label
    
    def _get_contrastive_pair(self, idx):
        """è·å–å¯¹æ¯”å­¦ä¹ çš„å›¾åƒå¯¹ - ä¼˜åŒ–è´Ÿæ ·æœ¬é€‰æ‹©ç­–ç•¥"""
        anchor_path = self.image_paths[idx]
        anchor_label = self.labels[idx]
        
        # åŠ¨æ€è°ƒæ•´æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼š60% è´Ÿæ ·æœ¬ï¼Œ40% æ­£æ ·æœ¬
        # è¿™æ ·æ¨¡å‹æ›´å¤šåœ°å­¦ä¹ åŒºåˆ†ä¸åŒç±»åˆ«
        is_positive = random.random() > 0.6
        
        if is_positive:
            # é€‰æ‹©åŒç±»çš„å¦ä¸€ä¸ªæ ·æœ¬ä½œä¸ºæ­£æ ·æœ¬
            positive_indices = [i for i in self.class_to_indices[anchor_label] if i != idx]
            if positive_indices:
                positive_idx = random.choice(positive_indices)
            else:
                positive_idx = idx  # å¦‚æœæ²¡æœ‰å…¶ä»–åŒç±»æ ·æœ¬ï¼Œä½¿ç”¨è‡ªå·±
            pair_path = self.image_paths[positive_idx]
            pair_label = anchor_label
            similarity = 1.0
        else:
            # æ™ºèƒ½è´Ÿæ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼šä»æ‰€æœ‰å…¶ä»–ç±»åˆ«ä¸­é€‰æ‹©
            negative_classes = [c for c in self.class_to_indices.keys() if c != anchor_label]
            if negative_classes:
                # ç­–ç•¥1: éšæœºé€‰æ‹©è´Ÿæ ·æœ¬ç±»åˆ«ï¼ˆä¿æŒå¤šæ ·æ€§ï¼‰
                if random.random() > 0.3:
                    negative_class = random.choice(negative_classes)
                    negative_idx = random.choice(self.class_to_indices[negative_class])
                # ç­–ç•¥2: é€‰æ‹©ç›¸é‚»IDä½œä¸ºå›°éš¾è´Ÿæ ·æœ¬ï¼ˆæ›´å…·æŒ‘æˆ˜æ€§ï¼‰
                else:
                    # ä¼˜å…ˆé€‰æ‹©ç›¸é‚»çš„ç”¨æˆ·IDä½œä¸ºå›°éš¾è´Ÿæ ·æœ¬
                    current_id = anchor_label
                    adjacent_ids = []
                    for neg_class in negative_classes:
                        id_diff = abs(neg_class - current_id)
                        if id_diff <= 2:  # ç›¸é‚»2ä¸ªIDå†…
                            adjacent_ids.append(neg_class)
                    
                    if adjacent_ids:
                        negative_class = random.choice(adjacent_ids)
                    else:
                        negative_class = random.choice(negative_classes)
                    
                    negative_idx = random.choice(self.class_to_indices[negative_class])
                
                pair_path = self.image_paths[negative_idx]
                pair_label = negative_class
            else:
                pair_path = anchor_path
                pair_label = anchor_label
            similarity = 0.0
        
        try:
            # åŠ è½½é”šç‚¹å›¾åƒå’Œé…å¯¹å›¾åƒ
            anchor_image = Image.open(anchor_path).convert('RGB')
            pair_image = Image.open(pair_path).convert('RGB')
            
            if self.transform:
                anchor_image = self.transform(anchor_image)
                pair_image = self.transform(pair_image)
            
            return (anchor_image, pair_image), (anchor_label, pair_label, similarity)
            
        except Exception as e:
            print(f"Error loading contrastive pair: {e}")
            # è¿”å›é›¶å›¾åƒå¯¹ä½œä¸ºå¤‡ç”¨
            if self.transform:
                zero_image = self.transform(Image.new('RGB', (256, 256), (0, 0, 0)))
            else:
                zero_image = transforms.ToTensor()(Image.new('RGB', (256, 256), (0, 0, 0)))
            return (zero_image, zero_image), (anchor_label, anchor_label, 1.0)

def load_dataset(dataset_path):
    """åŠ è½½æ•°æ®é›†"""
    image_paths = []
    labels = []
    class_names = []
    
    print("æ­£åœ¨æ‰«ææ•°æ®é›†...")
    
    # æ‰«ææ¯ä¸ªç”¨æˆ·ç›®å½•å¹¶æŒ‰æ•°å­—IDæ’åº
    user_dirs = []
    for class_dir in os.listdir(dataset_path):
        class_path = os.path.join(dataset_path, class_dir)
        if os.path.isdir(class_path) and class_dir.startswith('ID_'):
            try:
                # æå–æ•°å­—IDè¿›è¡Œæ’åº
                id_num = int(class_dir.split('_')[1])
                user_dirs.append((id_num, class_dir, class_path))
            except:
                continue
    
    # æŒ‰IDæ•°å­—æ’åº (1, 2, 3, ..., 10)
    user_dirs.sort(key=lambda x: x[0])
    
    for id_num, class_dir, class_path in user_dirs:
        class_names.append(class_dir)
        class_idx = len(class_names) - 1
        
        # æ‰«æè¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
        image_count = 0
        for img_file in os.listdir(class_path):
            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                img_path = os.path.join(class_path, img_file)
                image_paths.append(img_path)
                labels.append(class_idx)
                image_count += 1
        
        print(f"  {class_dir}: {image_count} å¼ å›¾åƒ")
    
    if len(class_names) == 0:
        print("é”™è¯¯: æœªæ‰¾åˆ°ç¬¦åˆæ ¼å¼çš„ç”¨æˆ·ç›®å½• (ID_*)")
        return [], [], []
    
    print(f"æ€»å…±æ‰¾åˆ° {len(class_names)} ä¸ªç”¨æˆ·ç±»åˆ«")
    print(f"ç±»åˆ«æ’åº: {class_names}")
    return image_paths, labels, class_names

def create_data_transforms():
    """åˆ›å»ºå¾®å¤šæ™®å‹’æ—¶é¢‘å›¾ä¸“ç”¨çš„ç®€æ´é¢„å¤„ç†"""
    
    # è®­ç»ƒæ—¶çš„æœ€å°åŒ–é¢„å¤„ç†ï¼ˆä¿æŒæ—¶é¢‘å›¾çš„ç‰©ç†æ„ä¹‰ï¼‰
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),  # ç›´æ¥resizeåˆ°ç›®æ ‡å°ºå¯¸
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # éªŒè¯/æµ‹è¯•æ—¶ä½¿ç”¨ç›¸åŒé¢„å¤„ç†
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”æŸå¤±å‡½æ•°"""
    
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, target):
        """
        Args:
            output1: ç¬¬ä¸€ä¸ªå›¾åƒçš„ç‰¹å¾å‘é‡
            output2: ç¬¬äºŒä¸ªå›¾åƒçš„ç‰¹å¾å‘é‡  
            target: ç›¸ä¼¼åº¦æ ‡ç­¾ (1è¡¨ç¤ºç›¸ä¼¼ï¼Œ0è¡¨ç¤ºä¸ç›¸ä¼¼)
        """
        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)
        
        loss_contrastive = torch.mean(
            target * torch.pow(euclidean_distance, 2) +
            (1 - target) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        )
        
        return loss_contrastive

class ResNet18Contrastive(nn.Module):
    """æ”¯æŒå¯¹æ¯”å­¦ä¹ çš„ResNet18æ¨¡å‹"""
    
    def __init__(self, num_classes, embedding_dim=128):
        super(ResNet18Contrastive, self).__init__()
        
        # åŠ è½½é¢„è®­ç»ƒçš„ResNet18
        self.backbone = models.resnet18(pretrained=True)
        
        # æ›´æ¿€è¿›çš„è§£å†»ç­–ç•¥ï¼šè§£å†»æ›´å¤šå±‚ä»¥æé«˜å­¦ä¹ èƒ½åŠ›
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # è§£å†»åé¢ä¸‰ä¸ªæ®‹å·®å— + BatchNormå±‚
        for param in self.backbone.layer2.parameters():
            param.requires_grad = True
        for param in self.backbone.layer3.parameters():
            param.requires_grad = True
        for param in self.backbone.layer4.parameters():
            param.requires_grad = True
        
        # è·å–ç‰¹å¾ç»´åº¦
        num_features = self.backbone.fc.in_features
        
        # ç§»é™¤åŸå§‹çš„åˆ†ç±»å¤´
        self.backbone.fc = nn.Identity()
        
        # æ·»åŠ ç‰¹å¾åµŒå…¥å±‚
        self.embedding = nn.Sequential(
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, embedding_dim),
            L2Norm(dim=1)  # L2æ ‡å‡†åŒ–ç”¨äºå¯¹æ¯”å­¦ä¹ 
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(embedding_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, x):
        features = self.backbone(x)
        embeddings = self.embedding(features)
        
        if self.training:
            # è®­ç»ƒæ—¶è¿”å›åµŒå…¥å‘é‡å’Œåˆ†ç±»ç»“æœ
            classification = self.classifier(embeddings)
            return embeddings, classification
        else:
            # æ¨ç†æ—¶åªè¿”å›åˆ†ç±»ç»“æœ
            return self.classifier(embeddings)

class L2Norm(nn.Module):
    """L2æ ‡å‡†åŒ–å±‚"""
    def __init__(self, dim=1):
        super(L2Norm, self).__init__()
        self.dim = dim
        
    def forward(self, x):
        return F.normalize(x, p=2, dim=self.dim)

def create_model(num_classes, contrastive_learning=False, embedding_dim=128):
    """åˆ›å»ºResNet18æ¨¡å‹"""
    
    if contrastive_learning:
        # ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¨¡å‹
        model = ResNet18Contrastive(num_classes, embedding_dim)
    else:
        # ä½¿ç”¨æ ‡å‡†åˆ†ç±»æ¨¡å‹
        model = models.resnet18(pretrained=True)
        
        # å†»ç»“å‰é¢çš„å±‚ï¼Œåªè®­ç»ƒæœ€åå‡ å±‚
        for param in model.parameters():
            param.requires_grad = False
        
        # è§£å†»æ›´å¤šå±‚è¿›è¡Œå¾®è°ƒ - ä¸ºè¾¾åˆ°100%å‡†ç¡®ç‡
        for name, param in model.named_parameters():
            if 'layer4' in name or 'layer3' in name or 'layer2' in name or 'fc' in name:
                param.requires_grad = True
        
        # æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚
        num_features = model.fc.in_features
        model.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    return model

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001, device='cuda', contrastive_learning=False, contrastive_weight=0.5):
    """è®­ç»ƒæ¨¡å‹"""
    
    model = model.to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    contrastive_criterion = ContrastiveLoss(margin=1.0) if contrastive_learning else None
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œæ›´å¹³æ»‘çš„å­¦ä¹ ç‡è¡°å‡
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs//4, eta_min=learning_rate*0.01)
    
    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []
    contrastive_losses = [] if contrastive_learning else None
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œè®¾å¤‡: {device}")
    print(f"å¯¹æ¯”å­¦ä¹ æ¨¡å¼: {'å¼€å¯' if contrastive_learning else 'å…³é—­'}")
    if contrastive_learning:
        print(f"å¯¹æ¯”å­¦ä¹ æƒé‡: {contrastive_weight}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_loader.dataset)}")
    print("-" * 50)
    
    best_val_accuracy = 0.0
    best_model_state = None
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        running_loss = 0.0
        running_contrastive_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')
        for batch_idx, batch_data in enumerate(train_pbar):
            
            if contrastive_learning:
                # å¯¹æ¯”å­¦ä¹ æ¨¡å¼
                (anchor_images, pair_images), (anchor_labels, pair_labels, similarities) = batch_data
                anchor_images = anchor_images.to(device)
                pair_images = pair_images.to(device)
                anchor_labels = anchor_labels.to(device)
                similarities = similarities.to(device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                anchor_embeddings, anchor_outputs = model(anchor_images)
                pair_embeddings, pair_outputs = model(pair_images)
                
                # åˆ†ç±»æŸå¤±
                classification_loss = criterion(anchor_outputs, anchor_labels)
                
                # å¯¹æ¯”æŸå¤±
                contrastive_loss = contrastive_criterion(anchor_embeddings, pair_embeddings, similarities)
                
                # æ€»æŸå¤±
                total_loss = (1 - contrastive_weight) * classification_loss + contrastive_weight * contrastive_loss
                
                total_loss.backward()
                optimizer.step()
                
                running_loss += total_loss.item()
                running_contrastive_loss += contrastive_loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, predicted = torch.max(anchor_outputs.data, 1)
                total_samples += anchor_labels.size(0)
                correct_predictions += (predicted == anchor_labels).sum().item()
                
            else:
                # æ ‡å‡†åˆ†ç±»æ¨¡å¼
                images, labels = batch_data
                images, labels = images.to(device), labels.to(device)
                
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_samples += labels.size(0)
                correct_predictions += (predicted == labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_accuracy = 100 * correct_predictions / total_samples
            postfix = {'Loss': f'{running_loss/(batch_idx+1):.4f}', 'Acc': f'{current_accuracy:.2f}%'}
            if contrastive_learning:
                postfix['ContLoss'] = f'{running_contrastive_loss/(batch_idx+1):.4f}'
            train_pbar.set_postfix(postfix)
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        epoch_train_loss = running_loss / len(train_loader)
        epoch_train_accuracy = 100 * correct_predictions / total_samples
        epoch_contrastive_loss = running_contrastive_loss / len(train_loader) if contrastive_learning else 0
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_running_loss = 0.0
        val_correct_predictions = 0
        val_total_samples = 0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')
            for images, labels in val_pbar:
                images, labels = images.to(device), labels.to(device)
                
                if contrastive_learning:
                    # éªŒè¯æ—¶åªä½¿ç”¨åˆ†ç±»è¾“å‡º
                    outputs = model(images)  # æ¨ç†æ¨¡å¼ä¸‹åªè¿”å›åˆ†ç±»ç»“æœ
                else:
                    outputs = model(images)
                    
                loss = criterion(outputs, labels)
                
                val_running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total_samples += labels.size(0)
                val_correct_predictions += (predicted == labels).sum().item()
                
                # æ›´æ–°è¿›åº¦æ¡
                current_val_accuracy = 100 * val_correct_predictions / val_total_samples
                val_pbar.set_postfix({
                    'Loss': f'{val_running_loss/(len(val_pbar)-len(val_loader)+len(val_pbar)):.4f}',
                    'Acc': f'{current_val_accuracy:.2f}%'
                })
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        epoch_val_loss = val_running_loss / len(val_loader)
        epoch_val_accuracy = 100 * val_correct_predictions / val_total_samples
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if epoch_val_accuracy > best_val_accuracy:
            best_val_accuracy = epoch_val_accuracy
            best_model_state = model.state_dict().copy()
        
        # è®°å½•å†å²
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_accuracy)
        val_losses.append(epoch_val_loss)
        val_accuracies.append(epoch_val_accuracy)
        if contrastive_learning:
            contrastive_losses.append(epoch_contrastive_loss)
        
        # æ‰“å°epochç»“æœ
        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%')
        if contrastive_learning:
            print(f'  Contrastive Loss: {epoch_contrastive_loss:.4f}')
        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%')
        print(f'  Best Val Acc: {best_val_accuracy:.2f}%')
        print('-' * 50)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"å·²åŠ è½½æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_val_accuracy:.2f}%)")
    
    history = {
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'val_losses': val_losses,
        'val_accuracies': val_accuracies,
        'best_val_accuracy': best_val_accuracy
    }
    
    if contrastive_learning:
        history['contrastive_losses'] = contrastive_losses
    
    return model, history

def evaluate_model(model, test_loader, class_names, device='cuda'):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - æä¾›è¯¦ç»†çš„æ¯ç±»åˆ«å‡†ç¡®åº¦æŠ¥å‘Š"""
    
    model.eval()
    all_preds = []
    all_labels = []
    class_correct = {}
    class_total = {}
    
    # åˆå§‹åŒ–æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡
    for i, class_name in enumerate(class_names):
        class_correct[i] = 0
        class_total[i] = 0
    
    with torch.no_grad():
        for data, target in tqdm(test_loader, desc='Evaluating'):
            data, target = data.to(device), target.to(device)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è·å–è¾“å‡º
            if hasattr(model, 'classifier'):
                # å¯¹æ¯”å­¦ä¹ æ¨¡å‹åœ¨æ¨ç†æ—¶åªè¿”å›åˆ†ç±»ç»“æœ
                output = model(data)
            else:
                # æ ‡å‡†ResNetæ¨¡å‹
                output = model(data)
            
            pred = output.argmax(dim=1, keepdim=True)
            all_preds.extend(pred.cpu().numpy().flatten())
            all_labels.extend(target.cpu().numpy())
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°
            for i in range(target.size(0)):
                label = target[i].item()
                prediction = pred[i].item()
                class_total[label] += 1
                if label == prediction:
                    class_correct[label] += 1
    
    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    accuracy = accuracy_score(all_labels, all_preds)
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    classification_rep = classification_report(
        all_labels, all_preds, 
        target_names=class_names,
        digits=4
    )
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µ
    conf_matrix = confusion_matrix(all_labels, all_preds)
    
    # ç”Ÿæˆè¯¦ç»†çš„æ¯ç±»åˆ«å‡†ç¡®åº¦æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æ¯ä¸ªç±»åˆ«è¯¦ç»†å‡†ç¡®åº¦æŠ¥å‘Š")
    print("="*60)
    
    for i, class_name in enumerate(class_names):
        if class_total[i] > 0:
            class_accuracy = 100.0 * class_correct[i] / class_total[i]
            print(f"{class_name:>8}: {class_correct[i]:>3}/{class_total[i]:>3} = {class_accuracy:>6.2f}% "
                  f"{'âœ…' if class_accuracy >= 90 else 'âš ï¸' if class_accuracy >= 70 else 'âŒ'}")
        else:
            print(f"{class_name:>8}: æ— æµ‹è¯•æ ·æœ¬")
    
    print("="*60)
    print(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy*100:.2f}%")
    
    # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
    class_accuracies = []
    for i, class_name in enumerate(class_names):
        if class_total[i] > 0:
            acc = 100.0 * class_correct[i] / class_total[i]
            class_accuracies.append((class_name, acc, class_total[i]))
    
    if class_accuracies:
        class_accuracies.sort(key=lambda x: x[1], reverse=True)
        best_class = class_accuracies[0]
        worst_class = class_accuracies[-1]
        
        print(f"\nğŸ† æœ€ä½³è¡¨ç°: {best_class[0]} ({best_class[1]:.2f}%, {best_class[2]}æ ·æœ¬)")
        print(f"âš ï¸  å¾…æ”¹è¿›: {worst_class[0]} ({worst_class[1]:.2f}%, {worst_class[2]}æ ·æœ¬)")
        
        # è®¡ç®—ç±»åˆ«é—´å‡†ç¡®ç‡å·®å¼‚
        acc_diff = best_class[1] - worst_class[1]
        print(f"ğŸ“ˆ ç±»åˆ«å·®å¼‚: {acc_diff:.2f}% ({'è‰¯å¥½' if acc_diff <= 10 else 'è¾ƒå¤§' if acc_diff <= 20 else 'æ˜¾è‘—'})")
    
    return accuracy * 100, classification_rep, conf_matrix

def plot_training_history(history, contrastive_learning=False):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    
    if contrastive_learning:
        fig, axes = plt.subplots(2, 3, figsize=(20, 10))
    else:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    axes[0, 0].plot(history['train_losses'], label='è®­ç»ƒæŸå¤±', color='blue')
    axes[0, 0].plot(history['val_losses'], label='éªŒè¯æŸå¤±', color='red')
    axes[0, 0].set_title('æ¨¡å‹æŸå¤±')
    axes[0, 0].set_xlabel('Epochs')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡
    axes[0, 1].plot(history['train_accuracies'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
    axes[0, 1].plot(history['val_accuracies'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
    axes[0, 1].set_title('æ¨¡å‹å‡†ç¡®ç‡')
    axes[0, 1].set_xlabel('Epochs')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    if contrastive_learning:
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        axes[0, 2].plot(history['contrastive_losses'], label='å¯¹æ¯”æŸå¤±', color='green')
        axes[0, 2].set_title('å¯¹æ¯”å­¦ä¹ æŸå¤±')
        axes[0, 2].set_xlabel('Epochs')
        axes[0, 2].set_ylabel('Contrastive Loss')
        axes[0, 2].legend()
        axes[0, 2].grid(True)
    
    # å­¦ä¹ æ›²çº¿
    epochs = range(1, len(history['train_losses']) + 1)
    axes[1, 0].plot(epochs, history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±')
    axes[1, 0].plot(epochs, history['val_losses'], 'r-', label='éªŒè¯æŸå¤±')
    axes[1, 0].set_title('å­¦ä¹ æ›²çº¿')
    axes[1, 0].set_xlabel('Epochs')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # å‡†ç¡®ç‡å˜åŒ–è¶‹åŠ¿
    axes[1, 1].plot(epochs, history['train_accuracies'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
    axes[1, 1].plot(epochs, history['val_accuracies'], 'r-', label='éªŒè¯å‡†ç¡®ç‡')
    axes[1, 1].axhline(y=history['best_val_accuracy'], color='g', linestyle='--', 
                      label=f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history["best_val_accuracy"]:.2f}%')
    axes[1, 1].set_title('å‡†ç¡®ç‡è¶‹åŠ¿')
    axes[1, 1].set_xlabel('Epochs')
    axes[1, 1].set_ylabel('Accuracy (%)')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    if contrastive_learning:
        # å¯¹æ¯”æŸå¤±è¶‹åŠ¿
        axes[1, 2].plot(epochs, history['contrastive_losses'], 'g-', label='å¯¹æ¯”æŸå¤±')
        axes[1, 2].set_title('å¯¹æ¯”æŸå¤±å˜åŒ–è¶‹åŠ¿')
        axes[1, 2].set_xlabel('Epochs')
        axes[1, 2].set_ylabel('Contrastive Loss')
        axes[1, 2].legend()
        axes[1, 2].grid(True)
    
    plt.tight_layout()
    filename = 'training_history_contrastive.png' if contrastive_learning else 'training_history.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜ä¸º '{filename}'")

def plot_confusion_matrix_results(conf_matrix, class_names):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º 'confusion_matrix.png'")

def final_test_model(model, dataset_path, class_names, device='cuda', samples_per_user=100):
    """æœ€ç»ˆæµ‹è¯•ï¼šä»æ¯ä¸ªç”¨æˆ·å…¨éƒ¨æ•°æ®ä¸­éšæœºæŠ½å–æŒ‡å®šæ•°é‡å›¾åƒè¿›è¡Œè¯†åˆ«"""
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    model = model.to(device)
    model.eval()
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    total_correct = 0
    total_samples = 0
    user_results = {}
    
    print(f"\nğŸ”¬ æœ€ç»ˆæµ‹è¯•ï¼šæ¯ç”¨æˆ·æŠ½å– {samples_per_user} å¼ å›¾åƒ")
    print("=" * 60)
    
    for user_idx, user_id in enumerate(class_names):
        user_dir = os.path.join(dataset_path, user_id)
        if not os.path.exists(user_dir):
            continue
            
        # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰å›¾åƒ
        user_images = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            user_images.extend(glob.glob(os.path.join(user_dir, ext)))
        
        # éšæœºæŠ½å–æ ·æœ¬
        if len(user_images) > samples_per_user:
            selected_images = random.sample(user_images, samples_per_user)
        else:
            selected_images = user_images
            
        correct = 0
        total = len(selected_images)
        
        # é€å¼ é¢„æµ‹
        with torch.no_grad():
            for img_path in selected_images:
                try:
                    image = Image.open(img_path).convert('RGB')
                    image_tensor = transform(image).unsqueeze(0).to(device)
                    
                    # è·å–é¢„æµ‹
                    if hasattr(model, 'embedding'):
                        # å¯¹æ¯”å­¦ä¹ æ¨¡å¼ - æ¨ç†æ—¶åªè¿”å›åˆ†ç±»ç»“æœ
                        outputs = model(image_tensor)
                    else:
                        # æ ‡å‡†ResNetæ¨¡å¼
                        outputs = model(image_tensor)
                    
                    pred = torch.argmax(outputs, dim=1).item()
                    
                    if pred == user_idx:
                        correct += 1
                        
                except Exception as e:
                    print(f"å¤„ç†å›¾åƒé”™è¯¯ {img_path}: {e}")
                    total -= 1
                    
        accuracy = (correct / total * 100) if total > 0 else 0
        user_results[user_id] = {
            'correct': correct,
            'total': total,
            'accuracy': accuracy
        }
        
        # æ˜¾ç¤ºç»“æœ
        status = "âœ…" if accuracy >= 95 else "âš ï¸" if accuracy >= 90 else "âŒ"
        print(f"    {user_id}: {correct:3d}/{total:3d} = {accuracy:6.2f}% {status}")
        
        total_correct += correct
        total_samples += total
    
    overall_accuracy = (total_correct / total_samples * 100) if total_samples > 0 else 0
    
    print("=" * 60)
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœ: {total_correct}/{total_samples} = {overall_accuracy:.2f}%")
    
    # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®è¡¨ç°
    best_user = max(user_results.items(), key=lambda x: x[1]['accuracy'])
    worst_user = min(user_results.items(), key=lambda x: x[1]['accuracy'])
    
    print(f"ğŸ† æœ€ä½³è¡¨ç°: {best_user[0]} ({best_user[1]['accuracy']:.2f}%)")
    print(f"âš ï¸  å¾…æ”¹è¿›: {worst_user[0]} ({worst_user[1]['accuracy']:.2f}%)")
    print(f"ğŸ“ˆ å‡†ç¡®ç‡å·®å¼‚: {best_user[1]['accuracy'] - worst_user[1]['accuracy']:.2f}%")
    
    return overall_accuracy, user_results

def save_model_for_web(model, class_names, save_path='../public/models/resnet18_identity'):
    """ä¿å­˜æ¨¡å‹ç”¨äºWebéƒ¨ç½²"""
    
    os.makedirs(save_path, exist_ok=True)
    
    # ä¿å­˜PyTorchæ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'class_names': class_names,
        'num_classes': len(class_names)
    }, os.path.join(save_path, 'resnet18_identity.pth'))
    
    print(f"PyTorchæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}/resnet18_identity.pth")
    
    # ä¿å­˜ç±»åˆ«æ˜ å°„
    class_mapping = {i: name for i, name in enumerate(class_names)}
    with open(os.path.join(save_path, 'class_mapping.json'), 'w') as f:
        json.dump(class_mapping, f, indent=2)
    
    print(f"ç±»åˆ«æ˜ å°„å·²ä¿å­˜åˆ°: {save_path}/class_mapping.json")
    
    # å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼ˆç”¨äºåç»­è½¬æ¢ä¸ºTensorFlow.jsï¼‰
    try:
        model.eval()
        
        # æ£€æŸ¥æ¨¡å‹è®¾å¤‡å¹¶ç¡®ä¿è¾“å…¥è¾“å‡ºåœ¨åŒä¸€è®¾å¤‡
        device = next(model.parameters()).device
        dummy_input = torch.randn(1, 3, 224, 224).to(device)
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPUè¿›è¡ŒONNXå¯¼å‡ºï¼ˆé¿å…è®¾å¤‡ä¸åŒ¹é…ï¼‰
        model_cpu = model.to('cpu')
        dummy_input_cpu = torch.randn(1, 3, 224, 224)
        
        onnx_path = os.path.join(save_path, 'resnet18_identity.onnx')
        
        torch.onnx.export(
            model_cpu,
            dummy_input_cpu,
            onnx_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        print(f"ONNXæ¨¡å‹å·²ä¿å­˜åˆ°: {onnx_path}")
        print("\næç¤º: ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è½¬æ¢ä¸ºTensorFlow.jsæ ¼å¼:")
        print(f"pip install onnx-tf tensorflowjs")
        print(f"onnx-tf convert -i {onnx_path} -o {save_path}/tf_model")
        print(f"tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model {save_path}/tf_model {save_path}")
        
    except Exception as e:
        print(f"ONNXå¯¼å‡ºå¤±è´¥: {e}")
        print("PyTorchæ¨¡å‹å·²ä¿å­˜ï¼Œå¯ä»¥æ‰‹åŠ¨è¿›è¡Œè½¬æ¢")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒResNet18èº«ä»½åˆ†ç±»å™¨')
    parser.add_argument('--dataset_path', type=str, default='../dataset', 
                       help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=80, 
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, 
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=0.0002, 
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--save_path', type=str, default='../public/models/resnet18_identity',
                       help='æ¨¡å‹ä¿å­˜è·¯å¾„')
    parser.add_argument('--contrastive', action='store_true',
                       help='å¯ç”¨å¯¹æ¯”å­¦ä¹ æ¨¡å¼')
    parser.add_argument('--contrastive_weight', type=float, default=0.5,
                       help='å¯¹æ¯”æŸå¤±æƒé‡ (0.0-1.0)')
    parser.add_argument('--embedding_dim', type=int, default=128,
                       help='å¯¹æ¯”å­¦ä¹ åµŒå…¥ç»´åº¦')
    parser.add_argument('--final_test_samples', type=int, default=100,
                       help='æœ€ç»ˆæµ‹è¯•æ—¶æ¯ç”¨æˆ·æŠ½å–çš„æ ·æœ¬æ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®é›†
    image_paths, labels, class_names = load_dataset(args.dataset_path)
    
    if len(image_paths) == 0:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        return
    
    print(f"æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»å›¾åƒæ•°: {len(image_paths)}")
    print(f"  ç±»åˆ«æ•°: {len(class_names)}")
    print(f"  ç±»åˆ«: {class_names}")
    
    # åˆ†å‰²æ•°æ®é›† (85% è®­ç»ƒ, 15% éªŒè¯ï¼Œæ— æµ‹è¯•é›† - å……åˆ†åˆ©ç”¨å°æ•°æ®é›†)
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        image_paths, labels, test_size=0.15, random_state=42, stratify=labels
    )
    
    print(f"æ•°æ®åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_paths)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_paths)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®å˜æ¢
    train_transform, val_transform = create_data_transforms()
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    if args.contrastive:
        # å¯¹æ¯”å­¦ä¹ æ¨¡å¼ï¼šè®­ç»ƒé›†ä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼ŒéªŒè¯/æµ‹è¯•é›†ä½¿ç”¨æ ‡å‡†æ¨¡å¼
        train_dataset = GaitDataset(train_paths, train_labels, train_transform, contrastive_mode=True)
        print("è®­ç»ƒé›†é…ç½®ä¸ºå¯¹æ¯”å­¦ä¹ æ¨¡å¼")
    else:
        train_dataset = GaitDataset(train_paths, train_labels, train_transform, contrastive_mode=False)
    
    val_dataset = GaitDataset(val_paths, val_labels, val_transform, contrastive_mode=False)
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(len(class_names), contrastive_learning=args.contrastive, embedding_dim=args.embedding_dim)
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ...")
    trained_model, history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=args.epochs,
        learning_rate=args.learning_rate,
        device=device,
        contrastive_learning=args.contrastive,
        contrastive_weight=args.contrastive_weight
    )
    
    # æœ€ç»ˆæµ‹è¯•ï¼šä»æ¯ç”¨æˆ·å…¨éƒ¨æ•°æ®éšæœºæŠ½å–æŒ‡å®šæ•°é‡è¿›è¡Œæµ‹è¯•
    print("è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    final_accuracy, final_results = final_test_model(
        trained_model, args.dataset_path, class_names, device, args.final_test_samples
    )
    
    # ç®€åŒ–çš„æ€§èƒ½æŠ¥å‘Š
    print(f"\n{'='*70}")
    print("ğŸ¯ ResNet18èº«ä»½è¯†åˆ«ç³»ç»Ÿ - æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š")
    print(f"{'='*70}")
    print(f"ğŸ“ˆ è®­ç»ƒå®Œæˆ: æœ€ä½³éªŒè¯å‡†ç¡®ç‡ {history['best_val_accuracy']:.2f}%")
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_accuracy:.2f}%")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history, contrastive_learning=args.contrastive)
    
    # ä¿å­˜æ¨¡å‹
    print("ä¿å­˜æ¨¡å‹...")
    save_model_for_web(trained_model, class_names, args.save_path)
    
    print(f"\n{'='*70}")
    print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜å¹¶å¯ç”¨äºéƒ¨ç½²")
    print(f"{'='*70}")

if __name__ == '__main__':
    main()
